{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-text-suburi",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maruhachi/work-colaboratory/blob/master/RNN-suburi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BTajoHc4al8",
        "colab_type": "text"
      },
      "source": [
        "RNNã‚’ç”¨ã„ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmhsCTw54gcg",
        "colab_type": "code",
        "outputId": "4fa92830-e882-45f6-8ec6-fbe9ec6cdbc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-with-keras-ja.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not create work tree dir 'deep-learning-with-keras-ja': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCvJdW7H7Nsa",
        "colab_type": "code",
        "outputId": "ea206d75-1b08-4a21-f057-e6a1b2ffbeff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "!ls -la \"/gdrive/My Drive/develop/twitter-2019-12-14-a38ba08b715a79675a4a7b065962453045a5501c5174e970eea2aa16acfc1ea7/tweet.js\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n",
            "-rw------- 1 root root 1607640 Dec 14 05:32 '/gdrive/My Drive/develop/twitter-2019-12-14-a38ba08b715a79675a4a7b065962453045a5501c5174e970eea2aa16acfc1ea7/tweet.js'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyjG3MC83h0",
        "colab_type": "code",
        "outputId": "1a618183-4507-4ba8-a70d-9c94e597f0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r /content/deep-learning-with-keras-ja/ch06/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 348kB 9.7MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.1MB 48kB/s \n",
            "\u001b[?25hCollecting h5py==2.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/b8/a63fcc840bba5c76e453dd712dbca63178a264c8990e0086b72965d4e954/h5py-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.4MB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from -r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 5)) (0.0)\n",
            "Collecting matplotlib==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/50/d1649dafaecc91e360b1ca8defebb25f865e29928a98bc7d42ba3b1350e5/matplotlib-2.1.1-cp36-cp36m-manylinux1_x86_64.whl (15.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.0MB 120kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (1.17.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (3.10.0)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.1MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn==0.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 5)) (0.21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (2.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (3.1.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn==0.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 5)) (0.14.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107221 sha256=9ba10e946d963f4e5751d2e108c4dc2c73324692aec2b6b72614d30c84f261f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: plotnine 0.5.1 has requirement matplotlib>=3.0.0, but you'll have matplotlib 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: h5py, keras, html5lib, bleach, tensorboard, tensorflow, matplotlib\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "  Found existing installation: matplotlib 3.1.2\n",
            "    Uninstalling matplotlib-3.1.2:\n",
            "      Successfully uninstalled matplotlib-3.1.2\n",
            "Successfully installed bleach-1.5.0 h5py-2.7.1 html5lib-0.9999999 keras-2.1.6 matplotlib-2.1.1 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "matplotlib",
                  "mpl_toolkits",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdazGKNt8ReZ",
        "colab_type": "code",
        "outputId": "45c18f65-8e22-45c4-d701-93b84bf53dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Activation, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "import codecs\n",
        "\n",
        "\n",
        "INPUT_FILE = \"/gdrive/My Drive/develop/twitter-2019-12-14-a38ba08b715a79675a4a7b065962453045a5501c5174e970eea2aa16acfc1ea7/result.txt\"\n",
        "\n",
        "# extract the input as a stream of characters\n",
        "print(\"Extracting text from input...\")\n",
        "with codecs.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [line.strip().lower() for line in f\n",
        "             if len(line) != 0]\n",
        "    text = \" \".join(lines)\n",
        "\n",
        "# creating lookup tables\n",
        "# Here chars is the number of features in our character \"vocabulary\"\n",
        "chars = set(text)\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# create inputs and labels from the text. We do this by stepping\n",
        "# through the text ${step} character at a time, and extracting a\n",
        "# sequence of size ${seqlen} and the next output char. For example,\n",
        "# assuming an input text \"The sky was falling\", we would get the\n",
        "# following sequence of input_chars and label_chars (first 5 only)\n",
        "#   The sky wa -> s\n",
        "#   he sky was ->\n",
        "#   e sky was  -> f\n",
        "#    sky was f -> a\n",
        "#   sky was fa -> l\n",
        "print(\"Creating input and label text...\")\n",
        "SEQLEN = 10\n",
        "STEP = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i:i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "# vectorize the input and label chars\n",
        "# Each row of the input is represented by seqlen characters, each\n",
        "# represented as a 1-hot encoding of size len(char). There are\n",
        "# len(input_chars) such rows, so shape(X) is (len(input_chars),\n",
        "# seqlen, nb_chars).\n",
        "# Each row of output is a single character, also represented as a\n",
        "# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
        "# nb_chars).\n",
        "print(\"Vectorizing input and label text...\")\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "# Build the model. We use a single RNN with a fully connected layer\n",
        "# to compute the most likely predicted output char\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
        "                    input_shape=(SEQLEN, nb_chars),\n",
        "                    unroll=True))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "# We train the model in batches and test output generated at each step\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Iteration #: {}\".format(iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "\n",
        "    # testing model\n",
        "    # randomly choose a row from input_chars, then use it to\n",
        "    # generate text from model for next 100 chars\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "    print(\"Generating from seed: {}\".format(test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for j, ch in enumerate(test_chars):\n",
        "            Xtest[0, j, char2index[ch]] = 1\n",
        "        pred = model.predict(Xtest, verbose=0)[0]\n",
        "        ypred = index2char[np.argmax(pred)]\n",
        "        print(ypred, end=\"\")\n",
        "        # move forward with test_chars + ypred\n",
        "        test_chars = test_chars[1:] + ypred\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting text from input...\n",
            "Creating input and label text...\n",
            "Vectorizing input and label text...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "==================================================\n",
            "Iteration #: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "97675/97675 [==============================] - 42s 432us/step - loss: 4.9860\n",
            "Generating from seed: http://bit\n",
            "http://bitoosaiaaaso  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§\n",
            "==================================================\n",
            "Iteration #: 1\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 426us/step - loss: 4.2289\n",
            "Generating from seed: ã«ã—ã¦èª¤é­”åŒ–ã›ã‚‹ã‹ã®\n",
            "ã«ã—ã¦èª¤é­”åŒ–ã›ã‚‹ã‹ã® #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  @lovelive_sife  rt @lovelive_sife  rt @lovelive_sife  rt @lovelive_sife  rt @lovelive_sif\n",
            "==================================================\n",
            "Iteration #: 2\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 422us/step - loss: 3.9109\n",
            "Generating from seed: ã§ãƒ“ãƒƒã‚¯ãƒªã—ãŸã å¾®\n",
            "ã§ãƒ“ãƒƒã‚¯ãƒªã—ãŸã å¾®ã†ã¯ã®ã‚¤ãƒ³ãƒˆ  aqours  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ã‚¹ãƒ¼ã‚¹5ã‚¹555555555555555555555555555555555555555555555555555555\n",
            "==================================================\n",
            "Iteration #: 3\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 423us/step - loss: 3.7033\n",
            "Generating from seed:   ã‚ªã‚¤ã‚ªã‚¤ ã‚¹ã‚¯ãƒ•\n",
            "  ã‚ªã‚¤ã‚ªã‚¤ ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã®ãƒªãƒ¼ãƒˆã‚’rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µã—æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³  å‚¬aqours  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ\n",
            "==================================================\n",
            "Iteration #: 4\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 430us/step - loss: 3.5546\n",
            "Generating from seed: â‘ @lovelive\n",
            "â‘ @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³  å‚¬a  ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒ£ãƒ³ãƒˆã‚’å‘¨å¹´  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£\n",
            "==================================================\n",
            "Iteration #: 5\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 429us/step - loss: 3.4402\n",
            "Generating from seed:  #aqours  \n",
            " #aqours  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼  ã‚¹ã‚¯ã‚¹ã‚¿å…¬å¼( @llas_staff: âœ¨#ã‚¹ã‚¯ã‚¹ã‚¿å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé–‹è¨­ã‚’è¨˜å¿µã—ã¾ã—ã¦rtã‚­\n",
            "==================================================\n",
            "Iteration #: 6\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 426us/step - loss: 3.3498\n",
            "Generating from seed: ã‹â€¦ï¼Ÿ  ã¯ã˜ã‚ã¦ã®\n",
            "ã‹â€¦ï¼Ÿ  ã¯ã˜ã‚ã¦ã®ã‹ã‚‰ãƒ¼ã„ã‚’ã‚‚ã†  ã“ã®ã¯@ https://t.co/bchttps://t.co/bchttps://t.co/bchttps://t.co/bchttps://t.co/bchttps://t.c\n",
            "==================================================\n",
            "Iteration #: 7\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 427us/step - loss: 3.2760\n",
            "Generating from seed: ãƒ©ãƒ– #ã„ã¨ã—ã  \n",
            "ãƒ©ãƒ– #ã„ã¨ã—ã  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã†ã£ã¡ãƒ¼ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»ãƒ»\n",
            "==================================================\n",
            "Iteration #: 8\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 427us/step - loss: 3.2139\n",
            "Generating from seed: ã‚µãƒ³ã‚·ãƒ£ã‚¤ãƒ³åŠ‡å ´ç‰ˆã®\n",
            "ã‚µãƒ³ã‚·ãƒ£ã‚¤ãƒ³åŠ‡å ´ç‰ˆã®ã¯ #lovelive  ã‚ã£ãŸã‚ƒã†ã“ã¡ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼\n",
            "==================================================\n",
            "Iteration #: 9\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 424us/step - loss: 3.1597\n",
            "Generating from seed: ã‚·ãƒ£ãƒ„ã€ã‚’ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆ\n",
            "ã‚·ãƒ£ãƒ„ã€ã‚’ãƒ—ãƒ¬ã‚¼ãƒ³ãƒˆï¼  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã†ã£ã¡ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼\n",
            "==================================================\n",
            "Iteration #: 10\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 423us/step - loss: 3.1156\n",
            "Generating from seed: ã£ã±ã„â™ªâ™ª http\n",
            "ã£ã±ã„â™ªâ™ª https://t.co/ymartassamera  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³   aqours 3rd ãƒ©ã‚¤ãƒ–ãƒ„ã‚¢ãƒ¼æ‹›å¾…åˆ¸ç­‰ãŒç·å‹¢5\n",
            "==================================================\n",
            "Iteration #: 11\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 422us/step - loss: 3.0802\n",
            "Generating from seed: ã‚‚ã—ã‚Œã‚“ ã¦ã‹æ›¸ãä¸‹\n",
            "ã‚‚ã—ã‚Œã‚“ ã¦ã‹æ›¸ãä¸‹ã•ã£ãŸã‚‰ãªã‚“ã ã‹ã‚‰ã—ã¾ã—ãŸ  ãƒ©ã‚¤ãƒ–ã«ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒ•ã‚©ãƒ­ãƒ¼ã—ã¦ #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§\n",
            "==================================================\n",
            "Iteration #: 12\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 423us/step - loss: 3.0491\n",
            "Generating from seed:  ã„ã„æ©Ÿä¼šã‚’è²°ã£ãŸã—\n",
            " ã„ã„æ©Ÿä¼šã‚’è²°ã£ãŸã—ã¾ã—ã¦ãªã‚“ã ã‘ã©â€¦ https://t.co/jchicexchttps://t.co/jchicexchttps://t.co/jchicexchttps://t.co/jchicexchttps:\n",
            "==================================================\n",
            "Iteration #: 13\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 422us/step - loss: 3.0273\n",
            "Generating from seed: ã”ã†  é¢¨é‚ªæ°—å‘³ã§ãƒ\n",
            "ã”ã†  é¢¨é‚ªæ°—å‘³ã§ãƒãƒ‰ã®ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ #lovelive  aqours  rt @anawastaff: ã€ãƒ•ã‚©ãƒ­ãƒ¼&amp;å¯¾è±¡ãƒ„ã‚¤ãƒ¼ãƒˆã‚’rtã§å¿œå‹Ÿã—ã‚ˆã†  ã‚¿ãƒ–ãƒ¬ã‚¤ãƒ‰ã‚’ãƒ•ã‚§ã‚¹ã«ãªã‚“ã‹â€¦  #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã†ã£ã¡ãƒ¼\n",
            "==================================================\n",
            "Iteration #: 14\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 417us/step - loss: 3.0119\n",
            "Generating from seed: ãƒˆã‚«ãƒ¼ãƒ‰â€¦  ä»Šæ—¥ã“\n",
            "ãƒˆã‚«ãƒ¼ãƒ‰â€¦  ä»Šæ—¥ã“ã®ã¯#ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã†ã£ã¡ãƒ¼ã„ã„ã„ã ã‘ã©â€¦  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼  ã‚¹ã‚¯ã‚¹ã‚¿å…¬å¼( @llas_staff: \n",
            "==================================================\n",
            "Iteration #: 15\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9985\n",
            "Generating from seed: ã‚‹ã€‚çµ‚ã‚ã£ãŸã‚‰ä½•é£Ÿã¹\n",
            "ã‚‹ã€‚çµ‚ã‚ã£ãŸã‚‰ä½•é£Ÿã¹ã‚‹ã€‚  ã†ã‹  @kawasena ert @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼  ã‚¹ã‚¯ã‚¹ã‚¿å…¬å¼( @llas_staff: âœ¨#ã‚¹ã‚¯ã‚¹ã‚¿\n",
            "==================================================\n",
            "Iteration #: 16\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9896\n",
            "Generating from seed: ãˆã¦ã»ã—ã„ã®ã§ã€ã‚‚ã£\n",
            "ãˆã¦ã»ã—ã„ã®ã§ã€ã‚‚ã£ã¦ãªã„ã‚“ã ã‚ã†  @kawasena ãã†ãªã‚“ã¨ã‹ã£ãŸã‹ã‚‰ï¼Ÿ https://t.co/bbbgbgmere  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£\n",
            "==================================================\n",
            "Iteration #: 17\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9758\n",
            "Generating from seed: wwwwwwwwww\n",
            "wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n",
            "==================================================\n",
            "Iteration #: 18\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 402us/step - loss: 2.9602\n",
            "Generating from seed: ã”ã¾æ²¹è²·ã‚ãªãã‚ƒâ€¦ \n",
            "ã”ã¾æ²¹è²·ã‚ãªãã‚ƒâ€¦  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’é–‹å‚¬ï¼  ã‚¹ã‚¯ã‚¹ã‚¿å…¬å¼( @llas_staff: âœ¨#ã‚¹ã‚¯ã‚¹ã‚¿å…¬å¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé–‹è¨­è¨˜å¿µrtã‚­ãƒ£ãƒ³ãƒšãƒ¼\n",
            "==================================================\n",
            "Iteration #: 19\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 401us/step - loss: 2.9444\n",
            "Generating from seed: ã¦ãªã‹ã£ãŸï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ\n",
            "ã¦ãªã‹ã£ãŸï¼Ÿï¼Ÿï¼Ÿï¼Ÿï¼Ÿ #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã•ã£ã¦ãã‚‰ã£ã¦ãŸã€‚  ã“ã®ã«ã®ãŒã«ã“ã‚Œã®ã‹ã£ãŸã‹ãªã£ã¨ã—ã¦ã„ãŸã€‚  ã“ã¡ã‚ƒã£ãŸã®ã¯ã‹ã£ãŸã®ã§ã™ãã¦ #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã•ã£ã¦ãã‚‰ã£ã¦ãŸã€‚  ã“ã®ã«ã®ãŒã«ã“ã‚Œã®ã‹ã£ãŸã‹ãªã£ã¨ã—ã¦\n",
            "==================================================\n",
            "Iteration #: 20\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 399us/step - loss: 2.9276\n",
            "Generating from seed: ã‚ãã€ã‚ã„ã«ã‚ƒ å…ƒæ°—\n",
            "ã‚ãã€ã‚ã„ã«ã‚ƒ å…ƒæ°—ã«ãªã£ã¦ã„ã„ã†â€¦  ã“ã‚Œãªã‚“ã ãŒã„ã‹ã‚‰ãªã„  ã«ãªã„ã„ã„ã ã£ãŸã€‚ã€‚  @kawasena ãã†ãªã®ã®win  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³\n",
            "==================================================\n",
            "Iteration #: 21\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9089\n",
            "Generating from seed:   å†æ”¾é€ã«ã¦èã„ã¨\n",
            "  å†æ”¾é€ã«ã¦èã„ã¨ã‚‹ #lovelive  rt @lovelive_sif: #ã‚¹ã‚¯ãƒ•ã‚§ã‚¹ã‚·ãƒªãƒ¼ã‚º5å‘¨å¹´ è¨˜å¿µ æ¯æ—¥å½“ãŸã‚‹rtã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ğŸ’—  aqours 3rd ãƒ©ã‚¤ãƒ–ãƒ„ã‚¢ãƒ¼æ‹›å¾…åˆ¸ç­‰ãŒç·å‹¢55555åæ§˜ã«æ¯æ—¥å½“ãŸ\n",
            "==================================================\n",
            "Iteration #: 22\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 398us/step - loss: 2.8911\n",
            "Generating from seed: https://t.\n",
            "https://t.co/plemerthete  https://t.co/plemerthete  https://t.co/plemerthete  https://t.co/plemerthete  https:\n",
            "==================================================\n",
            "Iteration #: 23\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 401us/step - loss: 2.8722\n",
            "Generating from seed: ã¿ã ãªã€œ #Î¼sic\n",
            "ã¿ã ãªã€œ #Î¼sichana #ãƒ©ãƒ–ãƒ©ã‚¤ãƒ–ä¸‰æ˜§  ã†ã£ã¡ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼\n",
            "==================================================\n",
            "Iteration #: 24\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 400us/step - loss: 2.8540\n",
            "Generating from seed: ã‚ã£ã¡ã‚ƒæŒ‡ã®èª¿å­ãŒè‰¯\n",
            "ã‚ã£ã¡ã‚ƒæŒ‡ã®èª¿å­ãŒè‰¯ã„ã¦ã¿ã‚“ãªã«ï¼Ÿ  ãªã‚“ã ã‹ã‚‰ã‚‰ã„ã„wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}