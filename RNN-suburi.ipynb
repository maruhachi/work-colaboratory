{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN-text-suburi",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maruhachi/work-colaboratory/blob/master/RNN-suburi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BTajoHc4al8",
        "colab_type": "text"
      },
      "source": [
        "RNNを用いたテキスト生成\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmhsCTw54gcg",
        "colab_type": "code",
        "outputId": "4fa92830-e882-45f6-8ec6-fbe9ec6cdbc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!git clone https://github.com/oreilly-japan/deep-learning-with-keras-ja.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: could not create work tree dir 'deep-learning-with-keras-ja': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCvJdW7H7Nsa",
        "colab_type": "code",
        "outputId": "ea206d75-1b08-4a21-f057-e6a1b2ffbeff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "!ls -la \"/gdrive/My Drive/develop/twitter-2019-12-14-a38ba08b715a79675a4a7b065962453045a5501c5174e970eea2aa16acfc1ea7/tweet.js\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n",
            "-rw------- 1 root root 1607640 Dec 14 05:32 '/gdrive/My Drive/develop/twitter-2019-12-14-a38ba08b715a79675a4a7b065962453045a5501c5174e970eea2aa16acfc1ea7/tweet.js'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxyjG3MC83h0",
        "colab_type": "code",
        "outputId": "1a618183-4507-4ba8-a70d-9c94e597f0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -r /content/deep-learning-with-keras-ja/ch06/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 9.7MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/c6/d08f7c549330c2acc1b18b5c1f0f8d9d2af92f54d56861f331f372731671/tensorflow-1.8.0-cp36-cp36m-manylinux1_x86_64.whl (49.1MB)\n",
            "\u001b[K     |████████████████████████████████| 49.1MB 48kB/s \n",
            "\u001b[?25hCollecting h5py==2.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/b8/a63fcc840bba5c76e453dd712dbca63178a264c8990e0086b72965d4e954/h5py-2.7.1-cp36-cp36m-manylinux1_x86_64.whl (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 56.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk==3.2.5 in /usr/local/lib/python3.6/dist-packages (from -r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 4)) (3.2.5)\n",
            "Requirement already satisfied: sklearn==0.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 5)) (0.0)\n",
            "Collecting matplotlib==2.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/50/d1649dafaecc91e360b1ca8defebb25f865e29928a98bc7d42ba3b1350e5/matplotlib-2.1.1-cp36-cp36m-manylinux1_x86_64.whl (15.0MB)\n",
            "\u001b[K     |████████████████████████████████| 15.0MB 120kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (1.3.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (1.17.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 1)) (3.13)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (3.10.0)\n",
            "Collecting tensorboard<1.9.0,>=1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/a6/0ae6092b7542cfedba6b2a1c9b8dceaf278238c39484f3ba03b03f07803c/tensorboard-1.8.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 51.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn==0.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 5)) (0.21.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (2.4.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==2.1.1->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (42.0.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.9.0,>=1.8.0->tensorflow==1.8.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 2)) (3.1.1)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 48.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn==0.0->-r /content/deep-learning-with-keras-ja/ch06/requirements.txt (line 5)) (0.14.1)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp36-none-any.whl size=107221 sha256=9ba10e946d963f4e5751d2e108c4dc2c73324692aec2b6b72614d30c84f261f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: plotnine 0.5.1 has requirement matplotlib>=3.0.0, but you'll have matplotlib 2.1.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: magenta 0.3.19 has requirement tensorflow>=1.12.0, but you'll have tensorflow 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: h5py, keras, html5lib, bleach, tensorboard, tensorflow, matplotlib\n",
            "  Found existing installation: h5py 2.8.0\n",
            "    Uninstalling h5py-2.8.0:\n",
            "      Successfully uninstalled h5py-2.8.0\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "  Found existing installation: matplotlib 3.1.2\n",
            "    Uninstalling matplotlib-3.1.2:\n",
            "      Successfully uninstalled matplotlib-3.1.2\n",
            "Successfully installed bleach-1.5.0 h5py-2.7.1 html5lib-0.9999999 keras-2.1.6 matplotlib-2.1.1 tensorboard-1.8.0 tensorflow-1.8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "keras",
                  "matplotlib",
                  "mpl_toolkits",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdazGKNt8ReZ",
        "colab_type": "code",
        "outputId": "45c18f65-8e22-45c4-d701-93b84bf53dd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "from keras.layers import Dense, Activation, SimpleRNN\n",
        "from keras.models import Sequential\n",
        "import codecs\n",
        "\n",
        "\n",
        "INPUT_FILE = \"/gdrive/My Drive/develop/twitter-2019-12-14-a38ba08b715a79675a4a7b065962453045a5501c5174e970eea2aa16acfc1ea7/result.txt\"\n",
        "\n",
        "# extract the input as a stream of characters\n",
        "print(\"Extracting text from input...\")\n",
        "with codecs.open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = [line.strip().lower() for line in f\n",
        "             if len(line) != 0]\n",
        "    text = \" \".join(lines)\n",
        "\n",
        "# creating lookup tables\n",
        "# Here chars is the number of features in our character \"vocabulary\"\n",
        "chars = set(text)\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "# create inputs and labels from the text. We do this by stepping\n",
        "# through the text ${step} character at a time, and extracting a\n",
        "# sequence of size ${seqlen} and the next output char. For example,\n",
        "# assuming an input text \"The sky was falling\", we would get the\n",
        "# following sequence of input_chars and label_chars (first 5 only)\n",
        "#   The sky wa -> s\n",
        "#   he sky was ->\n",
        "#   e sky was  -> f\n",
        "#    sky was f -> a\n",
        "#   sky was fa -> l\n",
        "print(\"Creating input and label text...\")\n",
        "SEQLEN = 10\n",
        "STEP = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i:i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "# vectorize the input and label chars\n",
        "# Each row of the input is represented by seqlen characters, each\n",
        "# represented as a 1-hot encoding of size len(char). There are\n",
        "# len(input_chars) such rows, so shape(X) is (len(input_chars),\n",
        "# seqlen, nb_chars).\n",
        "# Each row of output is a single character, also represented as a\n",
        "# dense encoding of size len(char). Hence shape(y) is (len(input_chars),\n",
        "# nb_chars).\n",
        "print(\"Vectorizing input and label text...\")\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "# Build the model. We use a single RNN with a fully connected layer\n",
        "# to compute the most likely predicted output char\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False,\n",
        "                    input_shape=(SEQLEN, nb_chars),\n",
        "                    unroll=True))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "# We train the model in batches and test output generated at each step\n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Iteration #: {}\".format(iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "\n",
        "    # testing model\n",
        "    # randomly choose a row from input_chars, then use it to\n",
        "    # generate text from model for next 100 chars\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "    print(\"Generating from seed: {}\".format(test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for j, ch in enumerate(test_chars):\n",
        "            Xtest[0, j, char2index[ch]] = 1\n",
        "        pred = model.predict(Xtest, verbose=0)[0]\n",
        "        ypred = index2char[np.argmax(pred)]\n",
        "        print(ypred, end=\"\")\n",
        "        # move forward with test_chars + ypred\n",
        "        test_chars = test_chars[1:] + ypred\n",
        "    print()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting text from input...\n",
            "Creating input and label text...\n",
            "Vectorizing input and label text...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "==================================================\n",
            "Iteration #: 0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/1\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "97675/97675 [==============================] - 42s 432us/step - loss: 4.9860\n",
            "Generating from seed: http://bit\n",
            "http://bitoosaiaaaso  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧\n",
            "==================================================\n",
            "Iteration #: 1\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 426us/step - loss: 4.2289\n",
            "Generating from seed: にして誤魔化せるかの\n",
            "にして誤魔化せるかの #ラブライブ三昧  @lovelive_sife  rt @lovelive_sife  rt @lovelive_sife  rt @lovelive_sife  rt @lovelive_sif\n",
            "==================================================\n",
            "Iteration #: 2\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 422us/step - loss: 3.9109\n",
            "Generating from seed: でビックリしたぞ 微\n",
            "でビックリしたぞ 微うはのイント  aqours  rt @lovelive_sif: #スクフェスシスース5ス555555555555555555555555555555555555555555555555555555\n",
            "==================================================\n",
            "Iteration #: 3\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 423us/step - loss: 3.7033\n",
            "Generating from seed:   オイオイ スクフ\n",
            "  オイオイ スクフェスのリートをrt @lovelive_sif: #スクフェスシリーズ5周年 記念し毎日当たるrtキャンペーン  催aqours  rt @lovelive_sif: #スクフェスシリーズ5周年 記念\n",
            "==================================================\n",
            "Iteration #: 4\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 430us/step - loss: 3.5546\n",
            "Generating from seed: ①@lovelive\n",
            "①@lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーン  催a  スクフェスシャントを周年  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャ\n",
            "==================================================\n",
            "Iteration #: 5\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 429us/step - loss: 3.4402\n",
            "Generating from seed:  #aqours  \n",
            " #aqours  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーンを開催！  スクスタ公式( @llas_staff: ✨#スクスタ公式アカウント開設を記念しましてrtキ\n",
            "==================================================\n",
            "Iteration #: 6\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 426us/step - loss: 3.3498\n",
            "Generating from seed: か…？  はじめての\n",
            "か…？  はじめてのからーいをもう  このは@ https://t.co/bchttps://t.co/bchttps://t.co/bchttps://t.co/bchttps://t.co/bchttps://t.c\n",
            "==================================================\n",
            "Iteration #: 7\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 427us/step - loss: 3.2760\n",
            "Generating from seed: ラブ #いとしき  \n",
            "ラブ #いとしき  #ラブライブ三昧  うっちー・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・・\n",
            "==================================================\n",
            "Iteration #: 8\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 42s 427us/step - loss: 3.2139\n",
            "Generating from seed: サンシャイン劇場版の\n",
            "サンシャイン劇場版のは #lovelive  あったゃうこちーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n",
            "==================================================\n",
            "Iteration #: 9\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 424us/step - loss: 3.1597\n",
            "Generating from seed: シャツ』をプレゼント\n",
            "シャツ』をプレゼント！  #ラブライブ三昧  うっちーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n",
            "==================================================\n",
            "Iteration #: 10\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 423us/step - loss: 3.1156\n",
            "Generating from seed: っぱい♪♪ http\n",
            "っぱい♪♪ https://t.co/ymartassamera  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーン   aqours 3rd ライブツアー招待券等が総勢5\n",
            "==================================================\n",
            "Iteration #: 11\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 422us/step - loss: 3.0802\n",
            "Generating from seed: もしれん てか書き下\n",
            "もしれん てか書き下さったらなんだからしました  ライブにアカウントをフォローして #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧  #ラブライブ三昧\n",
            "==================================================\n",
            "Iteration #: 12\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 423us/step - loss: 3.0491\n",
            "Generating from seed:  いい機会を貰ったし\n",
            " いい機会を貰ったしましてなんだけど… https://t.co/jchicexchttps://t.co/jchicexchttps://t.co/jchicexchttps://t.co/jchicexchttps:\n",
            "==================================================\n",
            "Iteration #: 13\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 422us/step - loss: 3.0273\n",
            "Generating from seed: ごう  風邪気味でバ\n",
            "ごう  風邪気味でバドのスクフェス #lovelive  aqours  rt @anawastaff: 【フォロー&amp;対象ツイートをrtで応募しよう  タブレイドをフェスになんか…  #ラブライブ三昧  うっちー\n",
            "==================================================\n",
            "Iteration #: 14\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 41s 417us/step - loss: 3.0119\n",
            "Generating from seed: トカード…  今日こ\n",
            "トカード…  今日このは#ラブライブ三昧  うっちーいいいだけど…  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーンを開催！  スクスタ公式( @llas_staff: \n",
            "==================================================\n",
            "Iteration #: 15\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9985\n",
            "Generating from seed: る。終わったら何食べ\n",
            "る。終わったら何食べる。  うか  @kawasena ert @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーンを開催！  スクスタ公式( @llas_staff: ✨#スクスタ\n",
            "==================================================\n",
            "Iteration #: 16\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9896\n",
            "Generating from seed: えてほしいので、もっ\n",
            "えてほしいので、もってないんだろう  @kawasena そうなんとかったから？ https://t.co/bbbgbgmere  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャ\n",
            "==================================================\n",
            "Iteration #: 17\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9758\n",
            "Generating from seed: wwwwwwwwww\n",
            "wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n",
            "==================================================\n",
            "Iteration #: 18\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 402us/step - loss: 2.9602\n",
            "Generating from seed: ごま油買わなきゃ… \n",
            "ごま油買わなきゃ…  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーンを開催！  スクスタ公式( @llas_staff: ✨#スクスタ公式アカウント開設記念rtキャンペー\n",
            "==================================================\n",
            "Iteration #: 19\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 401us/step - loss: 2.9444\n",
            "Generating from seed: てなかった？？？？？\n",
            "てなかった？？？？？ #ラブライブ三昧  さってくらってた。  このにのがにこれのかったかなっとしていた。  こちゃったのはかったのですぎて #ラブライブ三昧  さってくらってた。  このにのがにこれのかったかなっとして\n",
            "==================================================\n",
            "Iteration #: 20\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 399us/step - loss: 2.9276\n",
            "Generating from seed: あぁ、あいにゃ 元気\n",
            "あぁ、あいにゃ 元気になっていいう…  これなんだがいからない  にないいいだった。。  @kawasena そうなののwin  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャン\n",
            "==================================================\n",
            "Iteration #: 21\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 40s 405us/step - loss: 2.9089\n",
            "Generating from seed:   再放送にて聞いと\n",
            "  再放送にて聞いとる #lovelive  rt @lovelive_sif: #スクフェスシリーズ5周年 記念 毎日当たるrtキャンペーン💗  aqours 3rd ライブツアー招待券等が総勢55555名様に毎日当た\n",
            "==================================================\n",
            "Iteration #: 22\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 398us/step - loss: 2.8911\n",
            "Generating from seed: https://t.\n",
            "https://t.co/plemerthete  https://t.co/plemerthete  https://t.co/plemerthete  https://t.co/plemerthete  https:\n",
            "==================================================\n",
            "Iteration #: 23\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 401us/step - loss: 2.8722\n",
            "Generating from seed: みだな〜 #μsic\n",
            "みだな〜 #μsichana #ラブライブ三昧  うっちーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーーー\n",
            "==================================================\n",
            "Iteration #: 24\n",
            "Epoch 1/1\n",
            "97675/97675 [==============================] - 39s 400us/step - loss: 2.8540\n",
            "Generating from seed: めっちゃ指の調子が良\n",
            "めっちゃ指の調子が良いてみんなに？  なんだかららいいwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}